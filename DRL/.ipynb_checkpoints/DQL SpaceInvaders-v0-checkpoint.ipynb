{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space Invaders with DQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "#warnings.filterwarnings('ignore')  #ignores skimage warnings during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of our frame is:  Box(210, 160, 3)\n",
      "The action size is:  6\n",
      "Action Meanings:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
      "\n",
      "OH Actions:  [[1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "print(\"The size of our frame is: \", env.observation_space)\n",
    "print(\"The action size is: \", env.action_space.n)\n",
    "print(\"Action Meanings: \", env.get_action_meanings())\n",
    "#One Hot encoded version of our actions\n",
    "possible_actions = np.array(np.identity(env.action_space.n, dtype=int).tolist())\n",
    "print(\"\\nOH Actions: \", possible_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "1. Grayscale\n",
    "2. Crop \n",
    "3. Normalize pixel values\n",
    "4. Resize the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    gray = rgb2gray(frame)\n",
    "                 #[up: down, left:right]\n",
    "    cropped_frame = gray[8:-12, 4:-12]  #crop the lower part; trim from the sides\n",
    "    normalized_frame = cropped_frame / 255.0\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [110, 84])\n",
    "    return preprocessed_frame  #110x84x1 frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking frames\n",
    "We skip 4 frames each timestep i.e. we only stack every fourth frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4\n",
    "\n",
    "#Initialize deque with zero-images. One array for each image.\n",
    "stacked_frames = deque([np.zeros((110, 84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        #Clear all old frames\n",
    "        stacked_frames = deque([np.zeros((110, 84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        #Stack 'em\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    \n",
    "    return stacked_state, stacked_frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparams\n",
    "state_size = [110, 84, 4]\n",
    "action_size = env.action_space.n  #6 in our case\n",
    "learning_rate = 0.00025\n",
    "\n",
    "#Training Hyperparams\n",
    "total_episodes = 50\n",
    "max_steps = 50000\n",
    "batch_size = 64\n",
    "\n",
    "#Policy params\n",
    "explore_start = 1.0\n",
    "explore_stop = 0.01\n",
    "decay_rate = 1e-5\n",
    "gamma = 0.9  #for QL\n",
    "\n",
    "pretrain_length = batch_size  #No. of experiences stored in the memory when initialized at the first time\n",
    "memory_size = 1000000\n",
    "stack_size = 4\n",
    "training = True #False if you wanna see trained agent in action\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Deep Q net\n",
    " \n",
    " Stack of 4 frames as input, 3 conv layers, flatten, 2 FC\n",
    " \n",
    " Output a Q Value for each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name='inputs') #*state_size (star) indicates it is a tuple\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, self.action_size], name='actions_')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name='target')\n",
    "            \n",
    "            #ConvNet\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs_, filters=32, kernel_size=[8, 8], \n",
    "                                          strides=[4, 4], padding='VALID', \n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(), name='conv1')\n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name='conv1_out')\n",
    "            \n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1_out, filters=64, kernel_size=[4, 4],\n",
    "                                         strides=[2, 2], padding='VALID',\n",
    "                                         kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(), name='conv2')\n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name='conv2_out')\n",
    "            \n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2_out, filters= 64, kernel_size=[3, 3], \n",
    "                                         strides=[2, 2], padding='VALID', \n",
    "                                         kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(), name='conv3')\n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name='conv3_out')\n",
    "\n",
    "            self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs=self.flatten, units=512, activation=tf.nn.elu, kernel_initializer=tf.contrib.layers.xavier_initializer(), name='fc1')\n",
    "            self.output = tf.layers.dense(inputs=self.fc, kernel_initializer=tf.contrib.layers.xavier_initializer(), units=self.action_size, activation=None)\n",
    "            \n",
    "            #Q is the predicted value\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_))\n",
    "            #Loss is the difference between predicted vs. target Q values\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-6-20c556ab4a93>:15: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /home/ycee/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ycee/.local/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-20c556ab4a93>:30: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#Instantiate the Network\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size), size=batch_size, replace=False)\n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empty memory problem\n",
    "We pre-populate our memory by taking random, actions and storing experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(max_size=memory_size)\n",
    "for i in range(pretrain_length):  #batch_size\n",
    "    if i == 0:\n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    choice = random.randint(1, len(possible_actions)) - 1\n",
    "    action = possible_actions[choice]\n",
    "    #action = choice\n",
    "    next_state, reward, done, _ = env.step(choice)\n",
    "    \n",
    "    #env.render()\n",
    "    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "    \n",
    "    if done:\n",
    "        #if episode is over then there is no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        #add episode to memory when it finishes\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        #start a new episode\n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    else:\n",
    "        #if episode is still continuing\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        #new state is next state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('./tensorboard/dql/1')\n",
    "tf.summary.scalar('Loss', DQNetwork.loss)\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train now\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    exp_tradeoff = np.random.rand()\n",
    "    \n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if explore_probability > exp_tradeoff:\n",
    "        #make a random choice [EXPLORE]\n",
    "        choice = random.randint(1, len(possible_actions)) - 1\n",
    "        action = possible_actions[choice]\n",
    "        #action = choice\n",
    "    else: #[EXPLOIT]\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict={DQNetwork.inputs_:state.reshape((1, *state.shape))})\n",
    "\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[choice]\n",
    "        #action = choice\n",
    "        \n",
    "    return action, explore_probability\n",
    "\n",
    "def predict_action_e_greedy(state, actions):\n",
    "    epsilon = 0.2\n",
    "    if np.random.rand() < epsilon:#EXPLORE\n",
    "        choice = randon.randint(1, len(possible_actions)) - 1 #0-5\n",
    "        action = possible_actions[choice]\n",
    "    else:#EXPLOIT\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict={DQNetwork.inputs_:state.reshape((1, *state.shape))})\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[choice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 210.0 Explore P: 0.9932 Training Loss 0.0993\n",
      "Model Saved\n",
      "Episode: 1 Total reward: 110.0 Explore P: 0.9858 Training Loss 0.4786\n",
      "Episode: 2 Total reward: 120.0 Explore P: 0.9797 Training Loss 1.9359\n",
      "Episode: 3 Total reward: 180.0 Explore P: 0.9726 Training Loss 1.8997\n",
      "Episode: 4 Total reward: 185.0 Explore P: 0.9658 Training Loss 9.6552\n",
      "Episode: 5 Total reward: 105.0 Explore P: 0.9601 Training Loss 1.5480\n",
      "Model Saved\n",
      "Episode: 6 Total reward: 315.0 Explore P: 0.9467 Training Loss 3.4678\n",
      "Episode: 7 Total reward: 230.0 Explore P: 0.9380 Training Loss 625.0986\n",
      "Episode: 8 Total reward: 380.0 Explore P: 0.9268 Training Loss 0.0368\n",
      "Episode: 9 Total reward: 150.0 Explore P: 0.9208 Training Loss 18.5401\n",
      "Episode: 10 Total reward: 110.0 Explore P: 0.9152 Training Loss 0.1006\n",
      "Model Saved\n",
      "Episode: 11 Total reward: 60.0 Explore P: 0.9104 Training Loss 0.0327\n",
      "Episode: 12 Total reward: 55.0 Explore P: 0.9058 Training Loss 11.1173\n",
      "Episode: 13 Total reward: 120.0 Explore P: 0.8990 Training Loss 1.5526\n",
      "Episode: 14 Total reward: 210.0 Explore P: 0.8921 Training Loss 10.3950\n",
      "Episode: 15 Total reward: 120.0 Explore P: 0.8874 Training Loss 6.1698\n",
      "Model Saved\n",
      "Episode: 16 Total reward: 30.0 Explore P: 0.8840 Training Loss 0.0760\n",
      "Episode: 17 Total reward: 155.0 Explore P: 0.8781 Training Loss 13.8931\n",
      "Episode: 18 Total reward: 65.0 Explore P: 0.8746 Training Loss 7.6960\n",
      "Episode: 19 Total reward: 45.0 Explore P: 0.8703 Training Loss 13.8499\n",
      "Episode: 20 Total reward: 50.0 Explore P: 0.8670 Training Loss 3.4854\n",
      "Model Saved\n",
      "Episode: 21 Total reward: 260.0 Explore P: 0.8600 Training Loss 13.7031\n",
      "Episode: 22 Total reward: 100.0 Explore P: 0.8558 Training Loss 0.0039\n",
      "Episode: 23 Total reward: 105.0 Explore P: 0.8525 Training Loss 0.0801\n",
      "Episode: 24 Total reward: 185.0 Explore P: 0.8465 Training Loss 0.0952\n",
      "Episode: 25 Total reward: 110.0 Explore P: 0.8423 Training Loss 0.0053\n",
      "Model Saved\n",
      "Episode: 26 Total reward: 185.0 Explore P: 0.8355 Training Loss 1.5423\n",
      "Episode: 27 Total reward: 260.0 Explore P: 0.8278 Training Loss 1.5403\n",
      "Episode: 28 Total reward: 110.0 Explore P: 0.8237 Training Loss 0.0765\n",
      "Episode: 29 Total reward: 150.0 Explore P: 0.8184 Training Loss 0.0593\n",
      "Episode: 30 Total reward: 65.0 Explore P: 0.8140 Training Loss 3.4614\n",
      "Model Saved\n",
      "Episode: 31 Total reward: 215.0 Explore P: 0.8048 Training Loss 9.6342\n",
      "Episode: 32 Total reward: 180.0 Explore P: 0.7981 Training Loss 0.3921\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        #Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        decay_step = 0\n",
    "        rewards_list = []\n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            step = 0\n",
    "            \n",
    "            episode_rewards = []\n",
    "            state = env.reset()\n",
    "            #Process the state\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                decay_step += 1\n",
    "                \n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "                next_state, reward, done, _ = env.step(np.argmax(action))\n",
    "                \n",
    "                if episode_render:\n",
    "                    env.render()\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                if done:\n",
    "                    next_state = np.zeros((110, 84), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "                    #set step=max_steps to end the episode i.e end while loop\n",
    "                    step = max_steps\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "                    \n",
    "                    print('Episode: {}'.format(episode), 'Total reward: {}'.format(total_reward),\n",
    "                                  'Explore P: {:.4f}'.format(explore_probability),\n",
    "                                'Training Loss {:.4f}'.format(loss))\n",
    "                    rewards_list.append((episode, total_reward))\n",
    "                    # Store transition <st,at,rt+1,st+1> in memory D\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                else:\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    state = next_state\n",
    "                    \n",
    "                #LEARNING\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch]) \n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "                #Get Q values for next_state \n",
    "                Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                #Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "\n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
    "                                        feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                   DQNetwork.target_Q: targets_mb,\n",
    "                                                   DQNetwork.actions_: actions_mb})\n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                       DQNetwork.target_Q: targets_mb,\n",
    "                                                       DQNetwork.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "            # Save model every 5 episodes\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing our agent, watch it play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "************************************************\n",
      "\n",
      "EPISODE:  0\n",
      "Score 270.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    total_test_rewards = []\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    \n",
    "    for episode in range(1):\n",
    "        total_rewards = 0\n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        print(\"************************************************\\n\")\n",
    "        print(\"EPISODE: \", episode)\n",
    "        \n",
    "        while True:\n",
    "            #Reshape the state\n",
    "            state = state.reshape((1, *state_size))\n",
    "            #Get action from Qnet\n",
    "            Qs = sess.run(DQNetwork.output, feed_dict={DQNetwork.inputs_:state})\n",
    "            \n",
    "            #Take the biggest Q value\n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[choice]\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(np.argmax(action))\n",
    "            env.render()\n",
    "            \n",
    "            total_rewards += reward\n",
    "            \n",
    "            if done:\n",
    "                print (\"Score\", total_rewards)\n",
    "                total_test_rewards.append(total_rewards)\n",
    "                break\n",
    "                 \n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            state = next_state\n",
    "            \n",
    "    env.close()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
