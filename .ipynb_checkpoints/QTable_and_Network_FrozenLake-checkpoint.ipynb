{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Q Tables with FrozenLake-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space Discrete(16)\n",
      "Action space:  Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "print(\"Observation space\", env.observation_space)\n",
    "print(\"Action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Q-Table Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize with Os\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n]) # [16, 4] matrix with rows for states, cols for actions\n",
    "learning_rate = 0.8\n",
    "discount = 0.95\n",
    "num_episodes = 2000\n",
    "\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    s = env.reset() #gives first observation\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    \n",
    "    #Q-Table learning algorithm \n",
    "    while j < 99:\n",
    "        j += 1\n",
    "        #Choose a greedy action + noise\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1, env.action_space.n) * (1./(i+1)))\n",
    "        #Get new state and reward\n",
    "        s1, r, d, _ = env.step(a) #this is a greedy action\n",
    "        #Update the Q-table\n",
    "        Q[s, a] = Q[s, a] + learning_rate*(r + discount*np.max(Q[s1, :]) - Q[s, a])\n",
    "        rAll += r\n",
    "        s = s1  #Update to new state\n",
    "        if d == True:\n",
    "            break\n",
    "    rList.append(rAll)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.369\n",
      "\n",
      "Final Q-table values: \n",
      " [[1.72225274e-03 1.44678490e-03 5.78905101e-02 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 3.51131221e-04 3.66267346e-02]\n",
      " [5.39540164e-04 4.31081165e-04 1.05397709e-03 2.36448731e-02]\n",
      " [4.03126312e-04 5.80626680e-04 3.53212456e-04 1.95949509e-02]\n",
      " [6.32960702e-02 0.00000000e+00 0.00000000e+00 1.44678490e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.50833495e-04 1.20092335e-08 3.88851358e-05 1.54651619e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 3.87326450e-03 0.00000000e+00 1.35635738e-01]\n",
      " [0.00000000e+00 4.56046517e-01 0.00000000e+00 0.00000000e+00]\n",
      " [1.01509051e-01 0.00000000e+00 0.00000000e+00 7.65427698e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 6.88889153e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 2.37715329e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Score over time: \" + str(sum(rList) / num_episodes))\n",
    "print(\"\\nFinal Q-table values: \\n\", Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same with a Neural Net instead of a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = tf.placeholder(shape=[1, 16], dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([16, 4], 0, 0.01))\n",
    "Qout = tf.matmul(inputs1, W)\n",
    "predict = tf.argmax(Qout, 1)\n",
    "\n",
    "nextQ = tf.placeholder(shape=[1,4], dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "discount = 0.99\n",
    "epsilon = 0.1    #for epsilon greedy obv\n",
    "num_episodes = 2000\n",
    "\n",
    "jList = [] #step list\n",
    "rList = [] #reward list\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        while j < 99:\n",
    "            j += 1\n",
    "            \n",
    "            a, allQ = sess.run([predict, Qout], feed_dict={inputs1:np.identity(16)[s:s+1]})\n",
    "            if np.random.rand(1) < epsilon:\n",
    "                a[0] = env.action_space.sample()  #choose a random action with epsilon probability\n",
    "            #take a step\n",
    "            s1, r, d, _ = env.step(a[0])\n",
    "            #Obtain the Q' values by feeding the new state through our network\n",
    "            Q1 = sess.run(Qout, feed_dict={inputs1:np.identity(16)[s1:s1+1]})\n",
    "            #Obtain the maxQ' and set our target value for chosen action\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0, a[0]] = r + discount * maxQ1\n",
    "            #Training with target and predicted\n",
    "            _, W1 = sess.run([updateModel, W], feed_dict={inputs1:np.identity(16)[s:s+1], nextQ:targetQ})\n",
    "            \n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                #Reducing chance of random action as the model learns more and more\n",
    "                epsilon = 1./((i/50) + 10)\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of successful episodes: 0.1535%\n"
     ]
    }
   ],
   "source": [
    "print(\"Percent of successful episodes: \" + str(sum(rList)/num_episodes) + \"%\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note that the net performs worse than the tabular method. \n",
    "\n",
    "\"While neural networks allow for greater flexibility, they do so at the cost of stability when it comes to Q-Learning. There are a number of possible extensions to our simple Q-Network which allow for greater performance and more robust learning. \n",
    "Two tricks in particular are referred to as Experience Replay and Freezing Target Networks.\" _Juliani\n",
    "\n",
    "Personally I found that the %successful episodes increase the more you allow the agent to explore, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal experiment\n",
    "> Trying the FrozenLake8x8-v0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space Discrete(64)\n",
      "Action space:  Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v0')\n",
    "print(\"Observation space\", env.observation_space)\n",
    "print(\"Action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = tf.placeholder(shape=[1, 64], dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([64, 4], 0, 0.01))\n",
    "Qout = tf.matmul(inputs1, W)\n",
    "predict = tf.argmax(Qout, 1)\n",
    "\n",
    "nextQ = tf.placeholder(shape=[1, 4], dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "discount = 0.99\n",
    "epsilon = 0.1    #for epsilon greedy obv\n",
    "num_episodes = 2000\n",
    "\n",
    "jList = [] #step list\n",
    "rList = [] #reward list\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        while j < 99:\n",
    "            j += 1\n",
    "            \n",
    "            a, allQ = sess.run([predict, Qout], feed_dict={inputs1:np.identity(64)[s:s+1]})\n",
    "            if np.random.rand(1) < epsilon:\n",
    "                a[0] = env.action_space.sample()  #choose a random action with epsilon probability\n",
    "            #take a step\n",
    "            s1, r, d, _ = env.step(a[0])\n",
    "            #Obtain the Q' values by feeding the new state through our network\n",
    "            Q1 = sess.run(Qout, feed_dict={inputs1:np.identity(64)[s1:s1+1]})\n",
    "            #Obtain the maxQ' and set our target value for chosen action\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0, a[0]] = r + discount * maxQ1\n",
    "            #Training with target and predicted\n",
    "            _, W1 = sess.run([updateModel, W], feed_dict={inputs1:np.identity(64)[s:s+1], nextQ:targetQ})\n",
    "            \n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                #Reducing chance of random action as the model learns more and more\n",
    "                epsilon = 1./((i/50) + 10)\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of successful episodes: 0.002%\n"
     ]
    }
   ],
   "source": [
    "print(\"Percent of successful episodes: \" + str(sum(rList)/num_episodes) + \"%\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The network performs miserably on this one lol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-table on Frozenlake8x8-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize with Os\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n]) # [16, 4] matrix with rows for states, cols for actions\n",
    "learning_rate = 0.8\n",
    "discount = 0.95\n",
    "num_episodes = 2000\n",
    "\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    s = env.reset() #gives first observation\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    \n",
    "    #Q-Table learning algorithm \n",
    "    while j < 99:\n",
    "        j += 1\n",
    "        #Choose a greedy action + noise\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1, env.action_space.n) * (1./(i+1)))\n",
    "        #Get new state and reward\n",
    "        s1, r, d, _ = env.step(a) #this is a greedy action\n",
    "        #Update the Q-table\n",
    "        Q[s, a] = Q[s, a] + learning_rate*(r + discount*np.max(Q[s1, :]) - Q[s, a])\n",
    "        rAll += r\n",
    "        s = s1  #Update to new state\n",
    "        if d == True:\n",
    "            break\n",
    "    rList.append(rAll)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.2335\n",
      "\n",
      "Final Q-table values: \n",
      " [[5.89258784e-03 1.90954199e-02 6.21037321e-03 5.89206136e-03]\n",
      " [4.71582178e-03 6.35071688e-03 6.48522190e-03 3.24237006e-02]\n",
      " [3.65003764e-03 6.87068886e-03 2.74454356e-02 6.03281015e-03]\n",
      " [4.03627352e-03 5.70003788e-03 2.66952208e-02 4.72352928e-03]\n",
      " [4.23302392e-03 4.80856746e-03 3.77472666e-03 1.25066745e-01]\n",
      " [3.89839557e-03 6.51997244e-03 6.06387081e-02 4.28405761e-03]\n",
      " [5.07526432e-03 3.65509541e-03 1.49845511e-01 3.48639461e-03]\n",
      " [3.65526702e-03 3.74968582e-02 3.51107068e-03 0.00000000e+00]\n",
      " [1.02894194e-02 4.64238310e-03 5.20188441e-03 2.70464134e-02]\n",
      " [4.62731523e-03 3.65057254e-03 6.04874184e-03 2.85955768e-02]\n",
      " [2.30049962e-03 2.34578514e-03 3.50374117e-03 1.34476229e-01]\n",
      " [1.09897517e-04 1.20621193e-04 1.49190434e-04 1.75146630e-01]\n",
      " [4.12552424e-03 5.65187703e-03 6.54043660e-03 7.26611171e-02]\n",
      " [3.31293539e-03 5.03081210e-03 1.97909133e-01 3.98558179e-03]\n",
      " [3.62716973e-03 1.92264007e-01 3.68750405e-03 6.90361941e-03]\n",
      " [1.83259420e-04 1.68624853e-01 3.19355480e-03 3.50539802e-03]\n",
      " [3.23096100e-03 3.12562918e-03 3.64713401e-03 1.26723588e-02]\n",
      " [3.79504150e-03 3.24526883e-03 1.57755847e-02 2.70399974e-03]\n",
      " [1.49232883e-02 3.32870140e-04 4.41666765e-04 8.73460598e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.94205877e-04 2.06472176e-04 8.64168279e-02 7.49320166e-04]\n",
      " [6.86013610e-04 6.21931249e-04 0.00000000e+00 1.68541451e-01]\n",
      " [1.38392577e-03 3.02342108e-03 1.91323189e-01 3.04571000e-03]\n",
      " [0.00000000e+00 3.14761098e-03 2.86518648e-01 3.13372874e-03]\n",
      " [3.02137206e-03 2.09407881e-03 3.04083011e-03 7.81991590e-03]\n",
      " [3.11247070e-03 9.12409455e-04 2.16117711e-03 2.33426938e-02]\n",
      " [2.78648940e-03 2.20178856e-03 5.48753869e-04 1.40254993e-02]\n",
      " [6.02363543e-07 2.44035057e-03 2.12998998e-04 1.24047940e-04]\n",
      " [4.05718838e-03 5.41179317e-05 1.89978376e-04 1.19347662e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [7.18784346e-04 1.18225122e-05 2.76576968e-01 4.82261632e-05]\n",
      " [3.77642760e-01 0.00000000e+00 2.92728247e-03 3.15879859e-03]\n",
      " [2.03880040e-03 1.04163451e-03 2.95292777e-03 1.16851984e-03]\n",
      " [1.54040746e-02 4.38355240e-04 4.19279837e-04 6.31987314e-04]\n",
      " [4.25860529e-04 6.82402835e-05 2.95275232e-03 5.58522808e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.49326234e-03 2.33696352e-05 7.43366413e-04 4.66847560e-04]\n",
      " [1.41194400e-04 8.29688705e-02 6.41208800e-04 4.72676889e-04]\n",
      " [6.76275289e-04 1.05765703e-04 3.36464145e-04 5.53116381e-01]\n",
      " [1.83554924e-03 1.25271685e-03 7.73405621e-01 2.64160439e-03]\n",
      " [1.21302916e-03 1.03780127e-03 2.71880671e-04 3.27684940e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.18748537e-05 1.06746819e-05 3.12039845e-05]\n",
      " [2.55927367e-05 2.56245905e-05 1.81141999e-04 6.00024578e-04]\n",
      " [3.21175241e-04 4.03163031e-05 7.10007231e-04 1.93923148e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 8.79217643e-01 0.00000000e+00]\n",
      " [1.08982919e-03 1.88492356e-04 1.05715257e-03 1.75922918e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.86988397e-05 3.78199364e-05 2.32637854e-04 1.76426961e-08]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.23815854e-03 9.82148072e-01 0.00000000e+00]\n",
      " [1.62826717e-04 0.00000000e+00 2.05675852e-04 1.69593385e-04]\n",
      " [0.00000000e+00 2.23149191e-04 0.00000000e+00 1.69611163e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.01392360e-07 0.00000000e+00 1.71803852e-05 0.00000000e+00]\n",
      " [1.22896598e-04 1.44131835e-04 1.21789679e-01 1.81624389e-04]\n",
      " [1.85969113e-04 8.46255912e-05 1.01550709e-04 6.22136052e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Score over time: \" + str(sum(rList) / num_episodes))\n",
    "print(\"\\nFinal Q-table values: \\n\", Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7e5c19bcf8>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAF7tJREFUeJzt3XuQVOWZx/HvwwwXuQ4yIyC3AR2JqIngaEy8rInGgJuFJGYTqM3GZK2wWxuzSSV7IWXKpMz+EWOtqXLXTZZs3CRu1JjbhlJcvMSNuaEOeAVERuQygDAOlwGGYS48+0efwWbomTl9737796mamu7Tb3c/5+2eX59+33PmmLsjIiJhGVbsAkREJPcU7iIiAVK4i4gESOEuIhIghbuISIAU7iIiAVK4i4gESOEuIhIghbuISICqi/XEtbW1Xl9fX6ynFxEpS+vWrXvL3euGale0cK+vr6epqalYTy8iUpbMbHucdhqWEREJkMJdRCRACncRkQAp3EVEAqRwFxEJ0JDhbmb3mtk+M3tlgNvNzO42s2Yze8nMFuS+TBERSUecLfcfAAsHuX0R0BD9LAe+k31ZIiKSjSH3c3f3p82sfpAmS4AfeeJ8fWvNrMbMprr7nhzVGNuLOw8yzIyLpk/Iy+O3HOhgy74jvG/uWXl5/GT7j3bxzNY2Fl00Na37dfee4JfP7+JjC6YzbJidXL5l72H2H+3i3XMm5brUrD25aS/zzh7PwY5uOrp6uWTWxJTtHn5pN1edW8dvtrRyzdw6qsx47zd/zezaMVwyayIHjnbReuQ4o0dUMWp4Fb96YTcAc2rHMH/mRH6+voU5tWP40Dun8sruds6uGUVn9wlmnjmave2dPPvGfrp6T9B7wunsPsGo4cNoOXCM8yaP5Y4b38mxrl7uf3YHuw8eY3tbB3OnjOP8qePZffAYm988zJUNtRjwwLM7ufq8Wp7YtI/5M2vo7D7Bpj3t1E8azRXn1vLIy3u4aNoE3njrKO+bexZ/3NpG874jfOaKes6fOp72Y9186383c/eyi/mv32+j9fBxxoysZuak0fT0nuCllkPUTxrD3sOdHOns4aMLptPe2U3LgWNMqzmDzu5e2o91M2XCKH6xfhfzZ9bQcNZY1u84yJQJo3jHlHH852/fYMGsGp7fcZBr5tbhDrsPdXL0eA+d3b28Y8o4nti0j4tn1LDr4DHqJ42metgwbnpvPZv2tFM9zPjXXzfziUtnYAb72o/z7Lb97D/aBcAZw6s41t3Le+ZMYtLYERw61s318ybzxlsdfPCCyWzZd4QHnt3Bse5e3nvOJP7wehvjRlZzaf2Z7NjfwWMb9wLwvrl1/OV7ZvHX963j797fwIGObn7xfAud3b1ccU4tT766D4CrGmoZf8Zwnt7cSn3tGDa/eZhJY0cwd8o4jh7vYePudmZOGsNrew9z/bzJdHb38srudi6tn8ijr7zJGcOrmDtlHK2Hj3P1eXW8uPMgMyaOZu/hTt4xZTwbdh/iklkTWbf9ACOrh1EzegQ72jro7OnlvMnj2N52lJrRIzjY0cXcKeP5Q/NbtB3tYtTwYXz+/Q1MOGM433h4Iw2Tx/L+uWdx7fmTedeMmrz+XVmcc6hG4f6wu1+Y4raHgW+6+++i608C/+Tupx2hZGbLSWzdM3PmzEu2b4+1L35s9SseAWDbN/80p4/b58KvreHI8Z68PX6yG7/zB9ZtP0DTV6+jduzI2Pe7+8kt3PX4a3z7E+/iI/Onn1ye777JRv2KR5g8fiR7248DqWvc0dbB1Xc+xdkTRrH7UGfiD7TnBE+/1lrockWy9s8fvpBPXj4ro/ua2Tp3bxyqXUEnVN19pbs3untjXd2QR8+WnCPHewr2XC0HOgDo6U3vBOZtRxIBeaijO+c15VNfsA+ks6cXSGxdArzZ3snO/R15r0sk1/72mnMyDvZ05CLcdwEzkq5Pj5aJiEiR5CLcVwGfivaauRw4VIzxdhEReduQE6pm9gBwDVBrZi3A14DhAO7+XWA1cAPQDHQAn8lXsSIiEk+cvWWWDXG7A5/LWUUiIpI1HaEqIhIghbuISIAU7iIiAVK4i4gESOEuIhIghbuISIAU7iIiAVK4i4gESOEuIhIghbuISIAU7iIiAVK4i4gESOEuIhIghbuISIAU7iIiAVK4i4gESOEuIhIghbuISIAU7iIiAVK4S1ElTsErIrmmcBcRCZDCXUQkQAp3EZEAKdxFRAKkcJei0nyqSH4o3EVEAqRwFxEJkMJdRCRACncRkQAp3KWoNJ8qkh+xwt3MFprZZjNrNrMVKW6faWZPmdnzZvaSmd2Q+1JFRCSuIcPdzKqAe4BFwDxgmZnN69fsq8BD7j4fWAr8e64LFRGR+OJsuV8GNLv7VnfvAh4ElvRr48D46PIEYHfuShQRkXRVx2gzDdiZdL0FeHe/Nl8HHjOzzwNjgOtyUp2IiGQkVxOqy4AfuPt04AbgPjM77bHNbLmZNZlZU2tra46eWsqZ/uWvSH7ECfddwIyk69OjZcluBh4CcPc/AqOA2v4P5O4r3b3R3Rvr6uoyq1hERIYUJ9yfAxrMbLaZjSAxYbqqX5sdwLUAZnY+iXDXprmISJEMGe7u3gPcAqwBNpHYK2aDmd1uZoujZl8GPmtmLwIPAJ92fd8WESmaOBOquPtqYHW/ZbclXd4IXJHb0kREJFM6QlWKSl/vRPJD4S4iEiCFu4hIgBTuIiIBUriLiARI4S5FpR1mRfJD4R4oMyt2CXkV9tqJZE/hHqjQjyELe+1EsqdwFxEJkMJdRCRACncpKtcAi0heKNwDpQlVkcqmcA+UJlRFKpvCXUQkQAp3EZEAKdxLVN+oSroTjuU2XBF39ChVu9CHnkSyoXAXEQmQwr1E9e3sYmnuFxLqXiSpdv4JfY8gkWwo3EVEAqRwFxEJkMK9RFXKhGpcmlAVSY/CXUQkQAr3DBRii1ETqqfShKpIehTuIiIBUriLiARI4V6iKmVCVUeoiuSHwl1EJEAK9wwUYoMx07nCUKcYNaEqkh6Fu4hIgBTuIiIBUriXqJMTqmkOAZXbFGPcCWNNqIqkJ1a4m9lCM9tsZs1mtmKANh83s41mtsHM7s9tmSIiko7qoRqYWRVwD/ABoAV4zsxWufvGpDYNwFeAK9z9gJmdla+CS0Ehthc1oXoqTaiKpCfOlvtlQLO7b3X3LuBBYEm/Np8F7nH3AwDuvi+3ZYqISDrihPs0YGfS9ZZoWbLzgPPM7PdmttbMFqZ6IDNbbmZNZtbU2tqaWcUiIjKkXE2oVgMNwDXAMuB7ZlbTv5G7r3T3RndvrKury9FTi4hIf3HCfRcwI+n69GhZshZglbt3u/sbwGskwl6yFPr+INns8KK9ZUQGFifcnwMazGy2mY0AlgKr+rX5HxJb7ZhZLYlhmq05rFPSFPpkY9hrJ5K9IcPd3XuAW4A1wCbgIXffYGa3m9niqNkaoM3MNgJPAf/g7m35KrrYymGLsRxqzEbYayeSvSF3hQRw99XA6n7Lbku67MCXoh8RESkyHaEqIhIghXuJ0/BKfu4rEjqFe6A0oSpS2RTuGSiHLUZt8YtUNoW7iEiAFO4iIgFSuJe4wEdXsho+Cr1vRLKhcA+UJlRFKpvCPQPlsMWoCVWRyqZwFxEJkMJdRCRACncpquyOUNXgjMhAFO6B0oSqSGVTuGegHLYYNaEqUtkU7iIiAVK4i4gESOFe4gIfXcnyHKq5q0MkNAr3QGlCVaSyKdwzUA5bjJpQFalsCncRkQAp3EVEAqRwL3HlsE99VjShKpIXCvdAaUJVpLIp3EVEAqRwD5T2lhGpbAp3EZEAKdxLXOAb4LEnjEPvB5FcU7gHShOqIpVN4S4iEiCFewbKYYhAE6oilS1WuJvZQjPbbGbNZrZikHY3mpmbWWPuShQRkXQNGe5mVgXcAywC5gHLzGxeinbjgC8Az+S6yEoW+hZq3C8YqSZeQ/92IpKNOFvulwHN7r7V3buAB4ElKdp9A7gD6MxhfZIhTaiKVLY44T4N2Jl0vSVadpKZLQBmuPsjOaxNREQylPWEqpkNA+4Cvhyj7XIzazKzptbW1myfumjK4Z95hT5kEfbaiWQvTrjvAmYkXZ8eLeszDrgQ+D8z2wZcDqxKNanq7ivdvdHdG+vq6jKvWkREBhUn3J8DGsxstpmNAJYCq/pudPdD7l7r7vXuXg+sBRa7e1NeKq4w2gKP2qVoGHbPiGRnyHB39x7gFmANsAl4yN03mNntZrY43wVKZjShKlLZquM0cvfVwOp+y24boO012ZclIiLZ0BGqGSiHkRIN54hUNoW7iEiAFO4lLvQt1LjfMFJOqIbeOSJZULgHShOqIpVN4S4iEiCFewbKYTRAE6oilU3hLiISIIV7ierb8E53A7zctmhjH6Ga6l/+lt3aihSOwl1EJEAK9xKV6c4ufXfT3jIilU3hnoFymKwshxqzEfbaiWRP4S4iEiCFe4l6e8M7vW3UctuijX0OVR2hKpIWhbuISIAU7iVKE6qDC3vtRLKncBcRCZDCPQPlMNSrvWVEKpvCvURVzhGqmVdcbusqUkgKdxGRACncS5QmVAcX9tqJZE/hLiISIIV7BsphrlITqiKVTeFeok5OqKZ7v5xXkmc6QlUkLxTuIiIBUriXKE2oDi7stRPJnsJdRCRACvdMlMFYryZURSqbwr1EVc4RqnHbpWpZbmsrUjgKdxGRACncS5QmVAcX9tqJZE/hLiISoFjhbmYLzWyzmTWb2YoUt3/JzDaa2Utm9qSZzcp9qaUjm/9kWCiaUBWpbEOGu5lVAfcAi4B5wDIzm9ev2fNAo7u/E/gZ8K1cF1pp3j5CVedQHahd4J9fIlmJs+V+GdDs7lvdvQt4EFiS3MDdn3L3jujqWmB6bssUEZF0xAn3acDOpOst0bKB3Aw8muoGM1tuZk1m1tTa2hq/ygqkCdXBhb12ItnL6YSqmX0SaATuTHW7u69090Z3b6yrq8vlU4uISJLqGG12ATOSrk+Plp3CzK4DbgX+xN2P56a80lQOY72aUBWpbHG23J8DGsxstpmNAJYCq5IbmNl84D+Axe6+L/dlVp7KOUI1XsU6PlUkPUOGu7v3ALcAa4BNwEPuvsHMbjezxVGzO4GxwE/N7AUzWzXAw4mISAHEGZbB3VcDq/stuy3p8nU5rqviaUJ1cGGvnUj2dISqiEiAFO4iIgFSuGegEBN52U6olsveMvGPUD21oadYJiJvU7iLiARI4V6iNKE6uLDXTiR7CncRkQAp3EVEAqRwz0AhJvKy/Ze/5TLZGP8cqqdfL481FCkOhbuISIAU7iVKE6qDC3vtRLKncBcRCZDCXUQkQAr3DOgI1dyJW2f/Zp5imYi8TeEuIhIghXuJ0oTq4MJeO5HsKdxFRAKkcBcRCZDCPQPlMJFXLhOqmQp77USyp3CXoor/GXR6w9A/wESyoXAPlCZURSqbwl1EJEAKdxGRACncM5Duv+EthtDHo8NeO5HsKdxLXOAZHVuqflDXiAxM4R4oTaiKVDaFu4hIgBTuIiIBUrhnogwGezWhKlLZFO4lrhz2zMlG3M+glM3C7hqRrCjcA6UJVZHKFivczWyhmW02s2YzW5Hi9pFm9pPo9mfMrD7XhYqISHxDhruZVQH3AIuAecAyM5vXr9nNwAF3Pxf4NnBHrgsVEZH44my5XwY0u/tWd+8CHgSW9GuzBPhhdPlnwLUW+riAiEgJq47RZhqwM+l6C/Dugdq4e4+ZHQImAW/loshkDz23k+/9duugbT5w129y/bSn+MTKtVQPy+9n19724wB87v71jKquin2/LfuOAPDNR1/lv9duP+32fPdNurp7T5y2LFWNHV29p1x/seVQ3moSyaeRafw9ZyNOuOeMmS0HlgPMnDkzo8eoGT2chsljU952oKOLYWYD3p6tUcOreHnXIc6fOi4vj59s0tgRrN26n4umTUjrfrNrx/DYxr1cM7fulPOw9pxw3jzUmbe+yca2tg4unlFD6+HjHO7sHrDGXQePceW5tfyu+S2uaqhlZHUVT2zam/bz1U8azba2jlOWXXD2eDbsbh/wPlMnjGL6xDN4cechuqIPpHGjqhk9ourkBzHAgpk1rN9xMHYt1cOMnhOn7vbTOGsiTdsPMKduDFtbj6a83/hR1bR39sR6jvGjqjnec4LjPad/kAIsvXQGW/YdYd32A7HrzsSc2jG0HDxGlRn1tWPYtGfg/k4288zR7NjfMXTDIjj3rLG88dZRek/E23VrTt0Yll02I89VJcQJ911AcjXTo2Wp2rSYWTUwAWjr/0DuvhJYCdDY2JjRjmzXXzCF6y+YksldRUQqRpwx9+eABjObbWYjgKXAqn5tVgE3RZc/BvzaQz+KRkSkhA255R6Nod8CrAGqgHvdfYOZ3Q40ufsq4PvAfWbWDOwn8QEgIiJFEmvM3d1XA6v7Lbst6XIn8Oe5LU1ERDKlI1RFRAKkcBcRCZDCXUQkQAp3EZEAKdxFRAJkxdod3cxagdOPj4+nljz8a4McUF3pKdW6oHRrU13pCbGuWe5eN1SjooV7Nsysyd0bi11Hf6orPaVaF5RubaorPZVcl4ZlREQCpHAXEQlQuYb7ymIXMADVlZ5SrQtKtzbVlZ6Krassx9xFRGRw5brlLiIigyi7cB/qZN15fu4ZZvaUmW00sw1m9oVo+dfNbJeZvRD93JB0n69EtW42sw/msbZtZvZy9PxN0bIzzexxM9sS/Z4YLTczuzuq6yUzW5CnmuYm9ckLZtZuZl8sRn+Z2b1mts/MXklalnb/mNlNUfstZnZTqufKQV13mtmr0XP/0sxqouX1ZnYsqd++m3SfS6LXvzmqPatThQ1QV9qvW67/Xgeo6ydJNW0zsxei5YXsr4GyoXjvMXcvmx8S/3L4dWAOMAJ4EZhXwOefCiyILo8DXiNx0vCvA3+fov28qMaRwOyo9qo81bYNqO237FvAiujyCuCO6PINwKOAAZcDzxTotXsTmFWM/gKuBhYAr2TaP8CZwNbo98To8sQ81HU9UB1dviOprvrkdv0e59moVotqX5SHutJ63fLx95qqrn63/wtwWxH6a6BsKNp7rNy23OOcrDtv3H2Pu6+PLh8GNpE4f+xAlgAPuvtxd38DaCaxDoWSfOLyHwIfTlr+I09YC9SY2dQ813It8Lq7D3bgWt76y92fJnGugf7Pl07/fBB43N33u/sB4HFgYa7rcvfH3L3vHHprSZz9bEBRbePdfa0nEuJHSeuSs7oGMdDrlvO/18Hqira+Pw48MNhj5Km/BsqGor3Hyi3cU52se7BwzRszqwfmA89Ei26Jvl7d2/fVi8LW68BjZrbOEueqBZjs7nuiy28Ck4tQV5+lnPpHV+z+gvT7pxj99lcktvD6zDaz583sN2Z2VbRsWlRLIepK53UrdH9dBex19y1JywreX/2yoWjvsXIL95JgZmOBnwNfdPd24DvAOcDFwB4SXw0L7Up3XwAsAj5nZlcn3xhtoRRl1yhLnJ5xMfDTaFEp9Ncpitk/AzGzW4Ee4MfRoj3ATHefD3wJuN/MxhewpJJ73fpZxqkbEAXvrxTZcFKh32PlFu5xTtadV2Y2nMSL92N3/wWAu+919153PwF8j7eHEgpWr7vvin7vA34Z1bC3b7gl+r2v0HVFFgHr3X1vVGPR+yuSbv8UrD4z+zTwIeAvolAgGvZoiy6vIzGefV5UQ/LQTV7qyuB1K2R/VQMfBX6SVG9B+ytVNlDE91i5hXuck3XnTTSm931gk7vflbQ8ebz6I0DfTP4qYKmZjTSz2UADiYmcXNc1xszG9V0mMSH3CqeeuPwm4FdJdX0qmrG/HDiU9NUxH07Zoip2fyVJt3/WANeb2cRoSOL6aFlOmdlC4B+Bxe7ekbS8zsyqostzSPTP1qi2djO7PHqPfippXXJZV7qvWyH/Xq8DXnX3k8MtheyvgbKBYr7HspkhLsYPiVnm10h8Ct9a4Oe+ksTXqpeAF6KfG4D7gJej5auAqUn3uTWqdTNZzsgPUtccEnsivAhs6OsXYBLwJLAFeAI4M1puwD1RXS8DjXnsszFAGzAhaVnB+4vEh8seoJvEOObNmfQPiTHw5ujnM3mqq5nEuGvfe+y7Udsbo9f3BWA98GdJj9NIImxfB/6N6ADFHNeV9uuW67/XVHVFy38A/E2/toXsr4GyoWjvMR2hKiISoHIblhERkRgU7iIiAVK4i4gESOEuIhIghbuISIAU7iIiAVK4i4gESOEuIhKg/we0qlmoRGg3zAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
