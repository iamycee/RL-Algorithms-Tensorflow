{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Q Tables with FrozenLake-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space Discrete(16)\n",
      "Action space:  Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "print(\"Observation space\", env.observation_space)\n",
    "print(\"Action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Q-Table Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize with Os\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n]) # [16, 4] matrix with rows for states, cols for actions\n",
    "learning_rate = 0.8\n",
    "discount = 0.95\n",
    "num_episodes = 2000\n",
    "\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    s = env.reset() #gives first observation\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    \n",
    "    #Q-Table learning algorithm \n",
    "    while j < 99:\n",
    "        j += 1\n",
    "        #Choose a greedy action + noise\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1, env.action_space.n) * (1./(i+1)))\n",
    "        #Get new state and reward\n",
    "        s1, r, d, _ = env.step(a) #this is a greedy action\n",
    "        #Update the Q-table\n",
    "        Q[s, a] = Q[s, a] + learning_rate*(r + discount*np.max(Q[s1, :]) - Q[s, a])\n",
    "        rAll += r\n",
    "        s = s1  #Update to new state\n",
    "        if d == True:\n",
    "            break\n",
    "    rList.append(rAll)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.369\n",
      "\n",
      "Final Q-table values: \n",
      " [[1.72225274e-03 1.44678490e-03 5.78905101e-02 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 3.51131221e-04 3.66267346e-02]\n",
      " [5.39540164e-04 4.31081165e-04 1.05397709e-03 2.36448731e-02]\n",
      " [4.03126312e-04 5.80626680e-04 3.53212456e-04 1.95949509e-02]\n",
      " [6.32960702e-02 0.00000000e+00 0.00000000e+00 1.44678490e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.50833495e-04 1.20092335e-08 3.88851358e-05 1.54651619e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 3.87326450e-03 0.00000000e+00 1.35635738e-01]\n",
      " [0.00000000e+00 4.56046517e-01 0.00000000e+00 0.00000000e+00]\n",
      " [1.01509051e-01 0.00000000e+00 0.00000000e+00 7.65427698e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 6.88889153e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 2.37715329e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Score over time: \" + str(sum(rList) / num_episodes))\n",
    "print(\"\\nFinal Q-table values: \\n\", Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same with a Neural Net instead of a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = tf.placeholder(shape=[1, 16], dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([16, 4], 0, 0.01))\n",
    "Qout = tf.matmul(inputs1, W)\n",
    "predict = tf.argmax(Qout, 1)\n",
    "\n",
    "nextQ = tf.placeholder(shape=[1,4], dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "discount = 0.99\n",
    "epsilon = 0.1    #for epsilon greedy obv\n",
    "num_episodes = 2000\n",
    "\n",
    "jList = [] #step list\n",
    "rList = [] #reward list\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        while j < 99:\n",
    "            j += 1\n",
    "            \n",
    "            a, allQ = sess.run([predict, Qout], feed_dict={inputs1:np.identity(16)[s:s+1]})\n",
    "            if np.random.rand(1) < epsilon:\n",
    "                a[0] = env.action_space.sample()  #choose a random action with epsilon probability\n",
    "            #take a step\n",
    "            s1, r, d, _ = env.step(a[0])\n",
    "            #Obtain the Q' values by feeding the new state through our network\n",
    "            Q1 = sess.run(Qout, feed_dict={inputs1:np.identity(16)[s1:s1+1]})\n",
    "            #Obtain the maxQ' and set our target value for chosen action\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0, a[0]] = r + discount * maxQ1\n",
    "            #Training with target and predicted\n",
    "            _, W1 = sess.run([updateModel, W], feed_dict={inputs1:np.identity(16)[s:s+1], nextQ:targetQ})\n",
    "            \n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                #Reducing chance of random action as the model learns more and more\n",
    "                epsilon = 1./((i/50) + 10)\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of successful episodes: 0.1535%\n"
     ]
    }
   ],
   "source": [
    "print(\"Percent of successful episodes: \" + str(sum(rList)/num_episodes) + \"%\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note that the net performs worse than the tabular method. \n",
    "\n",
    "\"While neural networks allow for greater flexibility, they do so at the cost of stability when it comes to Q-Learning. There are a number of possible extensions to our simple Q-Network which allow for greater performance and more robust learning. \n",
    "Two tricks in particular are referred to as Experience Replay and Freezing Target Networks.\" _Juliani\n",
    "\n",
    "Personally I found that the %successful episodes increase the more you allow the agent to explore, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal experiment\n",
    "> Trying the FrozenLake8x8-v0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space Discrete(64)\n",
      "Action space:  Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v0')\n",
    "print(\"Observation space\", env.observation_space)\n",
    "print(\"Action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
