{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients on CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, math\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(4,), Discrete(2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the environment on random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for 1th episode was:  11.0\n",
      "Reward for 2th episode was:  16.0\n",
      "Reward for 3th episode was:  12.0\n",
      "Reward for 4th episode was:  23.0\n",
      "Reward for 5th episode was:  25.0\n",
      "Reward for 6th episode was:  13.0\n",
      "Reward for 7th episode was:  23.0\n",
      "Reward for 8th episode was:  11.0\n",
      "Reward for 9th episode was:  15.0\n",
      "Reward for 10th episode was:  39.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "random_episodes = 0\n",
    "reward_sum = 0\n",
    "while random_episodes < 10:\n",
    "    env.render()\n",
    "    observation, reward, done, _ = env.step(np.random.randint(0,2))\n",
    "    reward_sum += reward\n",
    "    if done: \n",
    "        print(\"Reward for \" + str(random_episodes + 1) + \"th episode was: \", reward_sum)\n",
    "        reward_sum= 0\n",
    "        random_episodes += 1\n",
    "        env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 10    #number of hidden layer neurons\n",
    "batch_size = 5    #update parameters after these many episodes\n",
    "learning_rate = 1e-2\n",
    "gamma = 0.99    #discount\n",
    "D = 4   #input states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/ycee/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ycee/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#The network gives the probability of taken an action (left or right) and takes state as input\n",
    "observations = tf.placeholder(tf.float32, [None, D], name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[D,H], initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations, W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1, W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32, [None, 1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32, name=\"reward_signal\")\n",
    "\n",
    "loglikelihood = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "loss = -tf.reduce_mean(loglikelihood * advantages)\n",
    "newGrads = tf.gradients(loss, tvars)\n",
    "\n",
    "#Apply gradients\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "W1Grad = tf.placeholder(tf.float32, name=\"batch_grad1\")\n",
    "W2Grad = tf.placeholder(tf.float32, name=\"batch_grad2\")\n",
    "\n",
    "batchGrad = [W1Grad, W2Grad]\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad, tvars))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage Function\n",
    "We want our agent to give more weight to the good actions. We do this by negatively weighing the actions that come towards the end of the episode as they most likely contributed to the end. Likewise, earlier episodes are seen as more positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):    #r is an array of rewards\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the agent in an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, hs, dlogps, drs, ys, tfps = [], [], [], [], [], []\n",
    "running_reward = None\n",
    "reward_sum = 0 \n",
    "episode_number = 1\n",
    "total_episodes = 10000\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episodes in this batch: 21.000000\n",
      "  Total average reward 21.000000.\n",
      "Average reward for episodes in this batch: 24.000000\n",
      "  Total average reward 21.000000.\n",
      "Average reward for episodes in this batch: 21.000000\n",
      "  Total average reward 21.000000.\n",
      "Average reward for episodes in this batch: 25.000000\n",
      "  Total average reward 21.000000.\n",
      "Average reward for episodes in this batch: 53.000000\n",
      "  Total average reward 21.000000.\n",
      "Average reward for episodes in this batch: 41.000000\n",
      "  Total average reward 21.000000.\n",
      "Average reward for episodes in this batch: 30.000000\n",
      "  Total average reward 21.000000.\n",
      "Average reward for episodes in this batch: 24.000000\n",
      "  Total average reward 21.000000.\n",
      "Average reward for episodes in this batch: 16.000000\n",
      "  Total average reward 21.000000.\n",
      "Average reward for episodes in this batch: 20.000000\n",
      "  Total average reward 21.000000.\n",
      "Average reward for episodes in this batch: 41.000000\n",
      "  Total average reward 22.000000.\n",
      "Average reward for episodes in this batch: 29.000000\n",
      "  Total average reward 22.000000.\n",
      "Average reward for episodes in this batch: 30.000000\n",
      "  Total average reward 22.000000.\n",
      "Average reward for episodes in this batch: 49.000000\n",
      "  Total average reward 22.000000.\n",
      "Average reward for episodes in this batch: 38.000000\n",
      "  Total average reward 22.000000.\n",
      "Average reward for episodes in this batch: 50.000000\n",
      "  Total average reward 22.000000.\n",
      "Average reward for episodes in this batch: 60.000000\n",
      "  Total average reward 23.000000.\n",
      "Average reward for episodes in this batch: 28.000000\n",
      "  Total average reward 23.000000.\n",
      "Average reward for episodes in this batch: 24.000000\n",
      "  Total average reward 23.000000.\n",
      "Average reward for episodes in this batch: 44.000000\n",
      "  Total average reward 23.000000.\n",
      "Average reward for episodes in this batch: 31.000000\n",
      "  Total average reward 23.000000.\n",
      "Average reward for episodes in this batch: 23.000000\n",
      "  Total average reward 23.000000.\n",
      "Average reward for episodes in this batch: 53.000000\n",
      "  Total average reward 23.000000.\n",
      "Average reward for episodes in this batch: 47.000000\n",
      "  Total average reward 24.000000.\n",
      "Average reward for episodes in this batch: 28.000000\n",
      "  Total average reward 24.000000.\n",
      "Average reward for episodes in this batch: 28.000000\n",
      "  Total average reward 24.000000.\n",
      "Average reward for episodes in this batch: 43.000000\n",
      "  Total average reward 24.000000.\n",
      "Average reward for episodes in this batch: 37.000000\n",
      "  Total average reward 24.000000.\n",
      "Average reward for episodes in this batch: 35.000000\n",
      "  Total average reward 24.000000.\n",
      "Average reward for episodes in this batch: 36.000000\n",
      "  Total average reward 24.000000.\n",
      "Average reward for episodes in this batch: 45.000000\n",
      "  Total average reward 24.000000.\n",
      "Average reward for episodes in this batch: 20.000000\n",
      "  Total average reward 24.000000.\n",
      "Average reward for episodes in this batch: 27.000000\n",
      "  Total average reward 24.000000.\n",
      "Average reward for episodes in this batch: 54.000000\n",
      "  Total average reward 25.000000.\n",
      "Average reward for episodes in this batch: 30.000000\n",
      "  Total average reward 25.000000.\n",
      "Average reward for episodes in this batch: 32.000000\n",
      "  Total average reward 25.000000.\n",
      "Average reward for episodes in this batch: 50.000000\n",
      "  Total average reward 25.000000.\n",
      "Average reward for episodes in this batch: 31.000000\n",
      "  Total average reward 25.000000.\n",
      "Average reward for episodes in this batch: 34.000000\n",
      "  Total average reward 25.000000.\n",
      "Average reward for episodes in this batch: 35.000000\n",
      "  Total average reward 25.000000.\n",
      "Average reward for episodes in this batch: 44.000000\n",
      "  Total average reward 26.000000.\n",
      "Average reward for episodes in this batch: 43.000000\n",
      "  Total average reward 26.000000.\n",
      "Average reward for episodes in this batch: 41.000000\n",
      "  Total average reward 26.000000.\n",
      "Average reward for episodes in this batch: 50.000000\n",
      "  Total average reward 26.000000.\n",
      "Average reward for episodes in this batch: 49.000000\n",
      "  Total average reward 26.000000.\n",
      "Average reward for episodes in this batch: 46.000000\n",
      "  Total average reward 27.000000.\n",
      "Average reward for episodes in this batch: 25.000000\n",
      "  Total average reward 27.000000.\n",
      "Average reward for episodes in this batch: 43.000000\n",
      "  Total average reward 27.000000.\n",
      "Average reward for episodes in this batch: 60.000000\n",
      "  Total average reward 27.000000.\n",
      "Average reward for episodes in this batch: 57.000000\n",
      "  Total average reward 27.000000.\n",
      "Average reward for episodes in this batch: 42.000000\n",
      "  Total average reward 27.000000.\n",
      "Average reward for episodes in this batch: 35.000000\n",
      "  Total average reward 28.000000.\n",
      "Average reward for episodes in this batch: 46.000000\n",
      "  Total average reward 28.000000.\n",
      "Average reward for episodes in this batch: 43.000000\n",
      "  Total average reward 28.000000.\n",
      "Average reward for episodes in this batch: 42.000000\n",
      "  Total average reward 28.000000.\n",
      "Average reward for episodes in this batch: 65.000000\n",
      "  Total average reward 28.000000.\n",
      "Average reward for episodes in this batch: 35.000000\n",
      "  Total average reward 28.000000.\n",
      "Average reward for episodes in this batch: 48.000000\n",
      "  Total average reward 29.000000.\n",
      "Average reward for episodes in this batch: 45.000000\n",
      "  Total average reward 29.000000.\n",
      "Average reward for episodes in this batch: 68.000000\n",
      "  Total average reward 29.000000.\n",
      "Average reward for episodes in this batch: 39.000000\n",
      "  Total average reward 29.000000.\n",
      "Average reward for episodes in this batch: 52.000000\n",
      "  Total average reward 30.000000.\n",
      "Average reward for episodes in this batch: 60.000000\n",
      "  Total average reward 30.000000.\n",
      "Average reward for episodes in this batch: 63.000000\n",
      "  Total average reward 30.000000.\n",
      "Average reward for episodes in this batch: 45.000000\n",
      "  Total average reward 30.000000.\n",
      "Average reward for episodes in this batch: 59.000000\n",
      "  Total average reward 31.000000.\n",
      "Average reward for episodes in this batch: 63.000000\n",
      "  Total average reward 31.000000.\n",
      "Average reward for episodes in this batch: 57.000000\n",
      "  Total average reward 31.000000.\n",
      "Average reward for episodes in this batch: 46.000000\n",
      "  Total average reward 31.000000.\n",
      "Average reward for episodes in this batch: 63.000000\n",
      "  Total average reward 32.000000.\n",
      "Average reward for episodes in this batch: 54.000000\n",
      "  Total average reward 32.000000.\n",
      "Average reward for episodes in this batch: 64.000000\n",
      "  Total average reward 32.000000.\n",
      "Average reward for episodes in this batch: 44.000000\n",
      "  Total average reward 32.000000.\n",
      "Average reward for episodes in this batch: 78.000000\n",
      "  Total average reward 33.000000.\n",
      "Average reward for episodes in this batch: 37.000000\n",
      "  Total average reward 33.000000.\n",
      "Average reward for episodes in this batch: 67.000000\n",
      "  Total average reward 33.000000.\n",
      "Average reward for episodes in this batch: 50.000000\n",
      "  Total average reward 33.000000.\n",
      "Average reward for episodes in this batch: 70.000000\n",
      "  Total average reward 34.000000.\n",
      "Average reward for episodes in this batch: 87.000000\n",
      "  Total average reward 34.000000.\n",
      "Average reward for episodes in this batch: 89.000000\n",
      "  Total average reward 35.000000.\n",
      "Average reward for episodes in this batch: 39.000000\n",
      "  Total average reward 35.000000.\n",
      "Average reward for episodes in this batch: 44.000000\n",
      "  Total average reward 35.000000.\n",
      "Average reward for episodes in this batch: 54.000000\n",
      "  Total average reward 35.000000.\n",
      "Average reward for episodes in this batch: 42.000000\n",
      "  Total average reward 35.000000.\n",
      "Average reward for episodes in this batch: 36.000000\n",
      "  Total average reward 35.000000.\n",
      "Average reward for episodes in this batch: 91.000000\n",
      "  Total average reward 36.000000.\n",
      "Average reward for episodes in this batch: 45.000000\n",
      "  Total average reward 36.000000.\n",
      "Average reward for episodes in this batch: 72.000000\n",
      "  Total average reward 36.000000.\n",
      "Average reward for episodes in this batch: 81.000000\n",
      "  Total average reward 37.000000.\n",
      "Average reward for episodes in this batch: 73.000000\n",
      "  Total average reward 37.000000.\n",
      "Average reward for episodes in this batch: 72.000000\n",
      "  Total average reward 37.000000.\n",
      "Average reward for episodes in this batch: 51.000000\n",
      "  Total average reward 37.000000.\n",
      "Average reward for episodes in this batch: 57.000000\n",
      "  Total average reward 38.000000.\n",
      "Average reward for episodes in this batch: 85.000000\n",
      "  Total average reward 38.000000.\n",
      "Average reward for episodes in this batch: 64.000000\n",
      "  Total average reward 38.000000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episodes in this batch: 86.000000\n",
      "  Total average reward 39.000000.\n",
      "Average reward for episodes in this batch: 99.000000\n",
      "  Total average reward 39.000000.\n",
      "Average reward for episodes in this batch: 76.000000\n",
      "  Total average reward 40.000000.\n",
      "Average reward for episodes in this batch: 63.000000\n",
      "  Total average reward 40.000000.\n",
      "Average reward for episodes in this batch: 100.000000\n",
      "  Total average reward 41.000000.\n",
      "Average reward for episodes in this batch: 59.000000\n",
      "  Total average reward 41.000000.\n",
      "Average reward for episodes in this batch: 90.000000\n",
      "  Total average reward 41.000000.\n",
      "Average reward for episodes in this batch: 133.000000\n",
      "  Total average reward 42.000000.\n",
      "Average reward for episodes in this batch: 74.000000\n",
      "  Total average reward 43.000000.\n",
      "Average reward for episodes in this batch: 81.000000\n",
      "  Total average reward 43.000000.\n",
      "Average reward for episodes in this batch: 93.000000\n",
      "  Total average reward 43.000000.\n",
      "Average reward for episodes in this batch: 77.000000\n",
      "  Total average reward 44.000000.\n",
      "Average reward for episodes in this batch: 86.000000\n",
      "  Total average reward 44.000000.\n",
      "Average reward for episodes in this batch: 85.000000\n",
      "  Total average reward 45.000000.\n",
      "Average reward for episodes in this batch: 89.000000\n",
      "  Total average reward 45.000000.\n",
      "Average reward for episodes in this batch: 71.000000\n",
      "  Total average reward 45.000000.\n",
      "Average reward for episodes in this batch: 72.000000\n",
      "  Total average reward 46.000000.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset()\n",
    "    \n",
    "    gradBuffer = sess.run(tvars)\n",
    "    for ix, grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad*0    #initially the gradient buffer is zero\n",
    "    \n",
    "    while episode_number <= total_episodes:\n",
    "        #Only render the environment once agent gets better\n",
    "        if reward_sum/batch_size > 100 or rendering == True:\n",
    "            env.render()\n",
    "            rendering = True\n",
    "        x = np.reshape(observation, [1, D])\n",
    "        \n",
    "        tfprob = sess.run(probability, feed_dict={observations:x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "        xs.append(x)\n",
    "        y = 1 if action == 0 else 0\n",
    "        ys.append(y)\n",
    "        \n",
    "        #take a step\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "        drs.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            episode_number += 1\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            tfp = tfps\n",
    "            xs, hs, dlogs, drs, ys, tfps = [], [], [], [], [], []    #reset array memory\n",
    "            \n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            # size the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "            discounted_epr -= np.mean(discounted_epr)  #subtract mean\n",
    "            discounted_epr //= np.std(discounted_epr)  #divide by SD\n",
    "            \n",
    "            #save gradient for this episode in the gradBuffer\n",
    "            tGrad = sess.run(newGrads, feed_dict={observations: epx, input_y:epy, advantages:discounted_epr})\n",
    "            \n",
    "            for ix, grad in enumerate(tGrad):\n",
    "                gradBuffer[ix] += grad\n",
    "                \n",
    "            # If we have completed enough episodes, then update the policy network with our gradients.\n",
    "            if episode_number % batch_size == 0: \n",
    "                sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                for ix,grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0   #make gradients 0 again as we train only on 5 episodes\n",
    "                    \n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                print('Average reward for episodes in this batch: %f\\n  Total average reward %f.' % (reward_sum//batch_size, running_reward//batch_size))\n",
    "                \n",
    "                if reward_sum//batch_size > 200: \n",
    "                    print(\"Task solved in\", episode_number,'episodes!')\n",
    "                    env.close()\n",
    "                    break\n",
    "                    \n",
    "                reward_sum = 0\n",
    "            \n",
    "            observation = env.reset()\n",
    "        \n",
    "print(episode_number,'Episodes completed.')\n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
